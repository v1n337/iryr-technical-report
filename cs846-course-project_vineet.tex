\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{url}
\usepackage{listings}

\begin{document}
\title{Semi-supervised Continuous Function Learning\\ for Sentiment Trend Detection in Big Data}


\author{
	\IEEEauthorblockN{Vineet John}
	\IEEEauthorblockA{
		David R. Cheriton School of Computer Science\\
		University of Waterloo\\
		Waterloo, Ontario N2L 3G1\\
		Email: vineet.john@uwaterloo.ca
	}
}

\maketitle

\begin{abstract}
Commercial establishments (like restaurants, service centers) have several sources of feedback, most of which need not be as structured as feedback provided by services like Yelp, or Amazon. Services like these provide a fine-grained score for product and service ratings. Some sources, however, like social media (Twitter, Facebook), mailing lists (Google Groups), forums (Quora) et al. are sources for much more volumnious, but unstructured and unlabeled data. This text could be pipelined into a system with a built-in prediction model, with the objective of generating real-time graphs on opinion and sentiment trends. Although such tasks like the one described about have been explored with respect to document classification problems in the past, the implementation described in this paper, by virtue of learning a continuous function rather than a discrete one, offers a lot more depth of insight as compared to document classification approaches. This study aims to explore the validity of such a continuous function predicting model to quantify sentiment about an entity, without the additional overhead of labeling, pre-processing, and feature extraction.
\end{abstract}


% no keywords
\providecommand{\keywords}[1]{\textbf{\textit{Keywords---}} #1}
\keywords{natural language processing, big data, word embedding, regression, time-series analytics}

\IEEEpeerreviewmaketitle

\section{Introduction}
Document labeling and attribute discovery is already a widely researched area in the domain of natural language processing. The most common use-cases of document classification are product \& service review rating predictions, automatic grading of essays, spam detection, plagiarism detection et al. However, most of the current approaches used for these tasks rely on lexicon based methods with a centralized source indicating the weights of a word towards a set of emotion or sentiment. Some other approaches use N-gram count vectorization approaches and document level similarity scores like TF-IDF. However, N-gram approaches only partially model the language context probabilities i.e. the probably of a word occurring next in the corpus depends only on the n-1 words that precede it. Word2Vec's model take additional word contexts into account, as described in section \ref{Word2Vec}.

However, models like these are difficult to train for the following reasons:
\begin{itemize}
  \item Having to hand-pick features
  \item Having to manually re-train the models on subset of the features to determine the best-fit
  \item Having to use stepwise regression to eliminate non-relevant features
\end{itemize}

All of the processes described above are time-consuming and tedious. The objective of this work is to implement and evaluate a strategy to effectively eliminate the above steps from the process of training a language model. The proposition is to rely on unsupervised strategies like continuous bag-of-words and the skip-gram model to learn word embeddings, that will serve as a syntactic and semantic proxy for the text being processed. The features thus learnt, can then be used to train regression models to predict text scores.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{images/word2vec_1.png}
\caption{Word2Vec Vector Space Intuition\cite{tensorflow_word2vec}}
\label{fig:word2vec-vectorspace-intuition}
\end{figure*}


\section{Similar Work}
This section discusses the previous work done on mining and analytics pertaining to text data.

Most of the similar work in the area of learning word-embeddings is related to classifying documents into a given set of labels or topics. The purpose of this study is to extend the usage of the document vector representation to demonstrating its usage when the response variable (the output of a learning task) a continuous function (document-scoring) rather than discrete values (document classification). This work can be viewed as a generalization of the previous approaches, because this study aims to predict fine-grained document scores, rather than coarse document classifications.

Most of the current work in usage of regression techniques on text content are related to metadata or features extraction \cite{su2015genetic} \cite{weissman2016natural}. Similarly there have been several studies to identify document tags using classification techniques\cite{bespalov2011sentiment}\cite{pang2002thumbs}. However, there is a relative absence of studies to evaluate the accuracy of regression models that try to predict a document score using a continuous-function model.

As far the the authors are aware, there has been no similar work to test the regression accuracy of paragraph vectors in an experimental research setup before. Hence, the evaluation of the effectiveness of this approach will be assessed using a generic evaluation metric.


\section{Problem Statement}
The problem statement can be formulated as an evaluation of Paragraph Vectors \cite{le2014distributed} on a document score regression task to evaluate the accuracy of the prediction model that can be built using the shallow neural network language model learning algorithm previously descibed by Mikolov et al. \cite{mikolov2013efficient}, which was primarily used to unsupervisedly train word embeddings.

The prediction scores will be evaluated using the co-efficient of determination metric, also known as the R\textsuperscript{2} metric \cite{cameron1997r}. The hypothesis is to prove that the document ratings have a positive correlation with the vectorized versions of the document text content, because the semantics of positive and negative reviews are expected to be captured by the shallow neural network trained on the corpus. The expected R\textsuperscript{2} metric score being over 0, will confirm the hypothesis, and a negative value will disprove it.


\section{Terminology}
This section formalizes the terminology that will be used throughout the paper.

The corpus of reviews on which the document vectors will be trained on will be referred to as `labeled documents'.

\section{Motivation}
The motivation for the project is to prove that a substantial amount of the semantic signal about sentiment with respect to a product or service, as extracted from a snippet or document of text, should prove to be a good indicator of the labeled document scores. If proven to be true, the objective includes building a real-time streaming framework, which can analyze unseen reviews from the previously provided exampled of unlabeled data, and be able to emit a score-prediction with minimal processing delays. This framework can be set-up on commodity hardware, and a given business can simply pipe their unlabeled reviews or articles into the system, and expect a graphical representation of the sentiment about a particular topic that is updated in real-time. The framework also allows rules to be set for alerts, so, for instance, if the approval rating of a particular business falls very low, an email alert could be triggered to the business decision makers to respond quicker to the negative sentiment, a process which would otherwise span several business days.

\subsection{Semi-supervised Learning}
The statistical learning formulation of the problem statement in this study, can be broken down into two distinct segments, namely:
\begin{itemize}
	\item Learning a projection of the document into vector space. This projection will act as a mathematical proxy for the sentiment that the document is trying to convey. This segment is completely unsupervised, and does not require any tuning or pre-processing to compute.
  \item Learning a relation between the aforementioned projection of each labeled document, and the associated score or label. This segment of the learning pipeline is supervised relies on previously scored or rated documents.
\end{itemize}

\subsection{Continuous function learning}
Assume that a given output variable \textit{y} depends on a variable \textit{x} such that
\begin{equation}
\displaystyle f(x) = \beta x + c + \epsilon
\end{equation}
\begin{equation}
\displaystyle y = f(x)
\end{equation}
where x represents a vector of independent variables, y represents the dependant or response variable, $\beta$ and c are model vector space functions that are applied to x to map it to y, and $\epsilon$ is the error term.

Now, assume that the variable \textit{x} increases by a small amount $\Delta x$, and that $x_0$ was the initial value, such that the value of \textit{x} is updated to
\begin{equation}
\displaystyle \Delta x = x - x_0
\end{equation}

A continuous function is defined as one in which an infinitely small change for the input variable x results in a corresponding infinitely small change in the response variable y \cite{continuous_function}, which can be expressed as
\begin{equation}
\displaystyle \Delta y = f(x) - f(x_0)
\end{equation}

Continuous functions are typically needed when the response variable calculated is a measurable quantity, rather than a class label. Since sentiment as fine-grained values offer more depth in terms of sentiment insight \cite{drake2008sentiment}, the study in this paper is formulated as a regression problem.

\section{Challenges}

\subsection{}


\section{Background}

\subsection{Word2Vec} \label{Word2Vec}
The Word2Vec model\cite{mikolov2013efficient} can take into consideration both preceding and succeeding words, to compute the probabililty of occurrence of the current word, as part of the Continuous Bag-of-words (CBOW) model. Alternatively, Word2Vec also provides a skip-gram model, which conversely, used the context of a word i.e. the words preceding and following, it, to predict the most statistically probably current word. These word embeddings can be used to derive both syntactic as well as symantic relations in the vector space of the language model. A representation of this is given in Figure \ref{fig:word2vec-vectorspace-intuition}.

\subsection{Doc2Vec}

\subsection{Statistical Learning Models}

\subsection{Scaling with Spark}


\section{System Architecture}


\section{Implementation}

\subsection{Offline learning model}

\subsection{Online real-time prediction engine}

\subsection{Tech stack}


\section{Evaluation}

\subsection{Testbed}

\subsection{Experimental Results}


\section{Conclusions}


\section{Future Work}


\section{Acknowledgments}
The authors would like to thank...


\bibliographystyle{abbrv}
\bibliography{cs846-course-project_vineet}

\end{document}
